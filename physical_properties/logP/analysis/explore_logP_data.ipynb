{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/env python\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "# Paths to input data.\n",
    "LOGP_SUBMISSIONS_DIR_PATH = 'analysis/logP_predictions'\n",
    "EXPERIMENTAL_DATA_FILE_PATH = './experimental_data/logP_experimental_values.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IgnoredSubmissionError(Exception):\n",
    "    \"\"\"Exception used to signal a submission that must be ignored.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BadFormatError(Exception):\n",
    "    \"\"\"Exception used to signal a submission with unexpected formatting.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class SamplSubmission:\n",
    "    \"\"\"A generic SAMPL submission.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the submission file.\n",
    "    Raises\n",
    "    ------\n",
    "    IgnoredSubmission\n",
    "        If the submission ID is among the ignored submissions.\n",
    "    \"\"\"\n",
    "    # The D3R challenge IDs that are handled by this class.\n",
    "    CHALLENGE_IDS = {1559}\n",
    "\n",
    "    # The IDs of the submissions used for testing the validation.\n",
    "    TEST_SUBMISSIONS = {}\n",
    "\n",
    "    # Section of the submission file.\n",
    "    SECTIONS = {}\n",
    "\n",
    "    # Sections in CSV format with columns names.\n",
    "    CSV_SECTIONS = {}\n",
    "    \n",
    "    def __init__(self, file_path, user_map):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        print(file_name)\n",
    "        file_data = file_name.split('-')\n",
    "\n",
    "        # Check if this is a deleted submission.\n",
    "        if file_data[0] == 'DELETED':\n",
    "            raise IgnoredSubmissionError('This submission was deleted.')\n",
    "\n",
    "        # Check if this is a test submission.\n",
    "        self.receipt_id = file_data[0]\n",
    "        if self.receipt_id in self.TEST_SUBMISSIONS:\n",
    "            raise IgnoredSubmissionError('This submission has been used for tests.')\n",
    "\n",
    "        # Check this is the correct challenge.\n",
    "        self.challenge_id = int(file_data[1])\n",
    "        assert self.challenge_id in self.CHALLENGE_IDS\n",
    "\n",
    "        # Store user map information.\n",
    "        user_map_record = user_map[user_map.receipt_id == self.receipt_id]\n",
    "        assert len(user_map_record) == 1\n",
    "        user_map_record = user_map_record.iloc[0]\n",
    "\n",
    "        self.id = user_map_record.id\n",
    "        self.participant = user_map_record.firstname + ' ' + user_map_record.lastname\n",
    "        self.participant_id = user_map_record.uid\n",
    "        #self.participant_email = user_map_record.email\n",
    "        #assert self.challenge_id == user_map_record.component\n",
    "    \n",
    "    @classmethod\n",
    "    def _read_lines(cls, file_path):\n",
    "        \"\"\"Generator to read the file and discard blank lines and comments.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            for line in f:\n",
    "                # Strip whitespaces.\n",
    "                line = line.strip()\n",
    "                # Don't return blank lines and comments.\n",
    "                if line != '' and line[0] != '#':\n",
    "                    yield line\n",
    "\n",
    "    @classmethod\n",
    "    def _load_sections(cls, file_path):\n",
    "        \"\"\"Load the data in the file and separate it by sections.\"\"\"\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        for line in cls._read_lines(file_path):\n",
    "            # Check if this is a new section.\n",
    "            if line[:-1] in cls.SECTIONS:\n",
    "                current_section = line[:-1]\n",
    "            else:\n",
    "                if current_section is None:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                try:\n",
    "                    sections[current_section].append(line)\n",
    "                except KeyError:\n",
    "                    sections[current_section] = [line]\n",
    "\n",
    "        # Check that all the sections have been loaded.\n",
    "        found_sections = set(sections.keys())\n",
    "        if found_sections != cls.SECTIONS:\n",
    "            raise BadFormatError('Missing sections: {}.'.format(found_sections - cls.SECTIONS))\n",
    "            \n",
    "    # Create a Pandas dataframe from the CSV format.\n",
    "        for section_name in cls.CSV_SECTIONS:\n",
    "            csv_str = io.StringIO('\\n'.join(sections[section_name]))\n",
    "            columns = cls.CSV_SECTIONS[section_name]\n",
    "            id_column = columns[0]\n",
    "            section = pd.read_csv(csv_str, index_col=id_column, names=columns, skipinitialspace=True)\n",
    "            #section = pd.read_csv(csv_str, names=columns, skipinitialspace=True)\n",
    "            sections[section_name] = section\n",
    "        return sections\n",
    "\n",
    "    @classmethod\n",
    "    def _create_comparison_dataframe(cls, column_name, submission_data, experimental_data):\n",
    "        \"\"\"Create a single dataframe with submission and experimental data.\"\"\"\n",
    "        # Filter only the systems IDs in this submissions.\n",
    "\n",
    "\n",
    "        experimental_data = experimental_data[experimental_data.index.isin(submission_data.index)] # match by column index\n",
    "        # Fix the names of the columns for labelling.\n",
    "        submission_series = submission_data[column_name]\n",
    "        submission_series.name += ' (calc)'\n",
    "        experimental_series = experimental_data[column_name]\n",
    "        experimental_series.name += ' (expt)'\n",
    "\n",
    "        # Concatenate the two columns into a single dataframe.\n",
    "        return pd.concat([experimental_series, submission_series], axis=1)\n",
    "\n",
    "class logPSubmission(SamplSubmission):\n",
    "    \"\"\"A submission for logP challenge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the submission file\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IgnoredSubmission\n",
    "        If the submission ID is among the ignored submissions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # The D3R challenge IDs that are handled by this class.\n",
    "    CHALLANGE_IDS = {1559}\n",
    "\n",
    "    # The IDs of the submissions that will be ignored in the analysis.\n",
    "    TEST_SUBMISSIONS = {}\n",
    "\n",
    "    # Section of the submission file.\n",
    "    SECTIONS = {'Predictions', 'Name', 'Software', 'Category', 'Method'}\n",
    "\n",
    "    # Sections in CSV format with columns names.\n",
    "    CSV_SECTIONS = {'Predictions': (\"Molecule ID\", \"logP mean\", \"logP SEM\", \"logP model uncertainty\")}\n",
    "\n",
    "\n",
    "    def __init__(self, file_path, user_map):\n",
    "        super().__init__(file_path, user_map)\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        file_data = file_name.split('-')\n",
    "\n",
    "        # Check if this is a type III submission\n",
    "        \n",
    "        self.submission_type = file_data[2]\n",
    "        assert self.submission_type in ['logP']\n",
    "\n",
    "        self.file_name, self.index = file_data[3:]\n",
    "        self.index = int(self.index)\n",
    "\n",
    "        # Load predictions.\n",
    "        sections = self._load_sections(file_path)  # From parent-class.\n",
    "        self.data = sections['Predictions']  # This is a pandas DataFrame.\n",
    "        self.name = sections['Name'][0]\n",
    "        self.category = sections['Category'][0] # New section for logP challenge.\n",
    "\n",
    "    def compute_logP_statistics(self, experimental_data, stats_funcs):\n",
    "        data = self._create_comparison_dataframe('logP mean', self.data, experimental_data)\n",
    "\n",
    "        # Create lists of stats functions to pass to compute_bootstrap_statistics.\n",
    "        stats_funcs_names, stats_funcs = zip(*stats_funcs.items())\n",
    "        bootstrap_statistics = compute_bootstrap_statistics(data.as_matrix(), stats_funcs, n_bootstrap_samples=10000)\n",
    "\n",
    "        # Return statistics as dict preserving the order.\n",
    "        return collections.OrderedDict((stats_funcs_names[i], bootstrap_statistics[i])\n",
    "                                      for i in range(len(stats_funcs)))\n",
    "\n",
    "    def compute_logP_model_uncertainty_statistics(self,experimental_data):\n",
    "\n",
    "        # Create a dataframe for data necessary for error slope analysis\n",
    "        expt_logP_series = experimental_data[\"logP mean\"]\n",
    "        expt_logP_SEM_series = experimental_data[\"logP SEM\"]\n",
    "        pred_logP_series = self.data[\"logP mean\"]\n",
    "        pred_logP_SEM_series = self.data[\"logP SEM\"]\n",
    "        pred_logP_mod_unc_series = self.data[\"logP model uncertainty\"]\n",
    "\n",
    "        # Concatenate the columns into a single dataframe.\n",
    "        data_exp =  pd.concat([expt_logP_series, expt_logP_SEM_series], axis=1)\n",
    "        data_exp = data_exp.rename(index=str, columns={\"logP mean\": \"logP mean (expt)\",\n",
    "                                                        \"logP SEM\": \"logP SEM (expt)\"})\n",
    "        \n",
    "        data_mod_unc = pd.concat([data_exp, pred_logP_series, pred_logP_SEM_series, pred_logP_mod_unc_series], axis=1)\n",
    "        data_mod_unc = data_mod_unc.rename(index=str, columns={\"logP mean (calc)\": \"logP mean (calc)\",\n",
    "                                                                \"logP SEM\": \"logP SEM (calc)\",\n",
    "                                                                \"logP model uncertainty\": \"logP model uncertainty\"})\n",
    "        #print(\"data_mod_unc:\\n\", data_mod_unc)\n",
    "\n",
    "        # Compute QQ-Plot Error Slope (ES)\n",
    "        calc = data_mod_unc.loc[:, \"logP mean (calc)\"].values\n",
    "        expt = data_mod_unc.loc[:, \"logP mean (expt)\"].values\n",
    "        dcalc = data_mod_unc.loc[:, \"logP model uncertainty\"].values\n",
    "        dexpt = data_mod_unc.loc[:, \"logP SEM (expt)\"].values\n",
    "        n_bootstrap_samples = 1000\n",
    "\n",
    "        X, Y, error_slope, error_slope_std, slopes = getQQdata(calc, expt, dcalc, dexpt, boot_its=n_bootstrap_samples)\n",
    "        #print(X)\n",
    "        #print(Y)\n",
    "        #print(\"ES:\", error_slope)\n",
    "        #print(\"ES std:\", error_slope_std)\n",
    "        #print(\"Bootstrapped Error Slopes:\", slopes)\n",
    "        QQplot_data = [X, Y, error_slope]\n",
    "\n",
    "        # Compute 95% confidence intervals of Error Slope\n",
    "        percentile = 0.95\n",
    "        percentile_index = int(np.floor(n_bootstrap_samples * (1 - percentile) / 2)) - 1\n",
    "\n",
    "        #for stats_func_idx, samples_statistics in enumerate(bootstrap_samples_statistics):\n",
    "        samples_statistics = np.asarray(slopes)\n",
    "        samples_statistics.sort()\n",
    "        stat_lower_percentile = samples_statistics[percentile_index]\n",
    "        stat_higher_percentile = samples_statistics[-percentile_index + 1]\n",
    "        confidence_interval = (stat_lower_percentile, stat_higher_percentile)\n",
    "\n",
    "        model_uncertainty_statistics = [error_slope, confidence_interval, samples_statistics]\n",
    "        \n",
    "        return model_uncertainty_statistics, QQplot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(directory_path, user_map):\n",
    "    submissions = []\n",
    "    for file_path in glob.glob(os.path.join(directory_path, '*.csv')):\n",
    "        try:\n",
    "            submission = logPSubmission(file_path, user_map)\n",
    "\n",
    "        except IgnoredSubmissionError:\n",
    "            continue\n",
    "        submissions.append(submission)\n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental data: \n",
      "              logP mean  logP SEM  Assay Type  Experimental ID  \\\n",
      "Molecule ID                                                     \n",
      "SM02              4.09      0.03         NaN              NaN   \n",
      "SM04              3.98      0.03         NaN              NaN   \n",
      "SM07              3.21      0.04         NaN              NaN   \n",
      "SM08              3.10      0.03         NaN              NaN   \n",
      "SM09              3.03      0.07         NaN              NaN   \n",
      "SM11              2.10      0.04         NaN              NaN   \n",
      "SM12              3.83      0.03         NaN              NaN   \n",
      "SM13              2.92      0.04         NaN              NaN   \n",
      "SM14              1.95      0.03         NaN              NaN   \n",
      "SM15              3.07      0.03         NaN              NaN   \n",
      "SM16              2.62      0.01         NaN              NaN   \n",
      "\n",
      "             Isomeric SMILES Molecule ID  \n",
      "Molecule ID                               \n",
      "SM02                     NaN        SM02  \n",
      "SM04                     NaN        SM04  \n",
      "SM07                     NaN        SM07  \n",
      "SM08                     NaN        SM08  \n",
      "SM09                     NaN        SM09  \n",
      "SM11                     NaN        SM11  \n",
      "SM12                     NaN        SM12  \n",
      "SM13                     NaN        SM13  \n",
      "SM14                     NaN        SM14  \n",
      "SM15                     NaN        SM15  \n",
      "SM16                     NaN        SM16  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../predictions/SAMPL6-user-map-logP.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-95812c8ec730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Import user map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../predictions/SAMPL6-user-map-logP.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0muser_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../predictions/SAMPL6-user-map-logP.csv'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SOME ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "\n",
    "# Read experimental data.\n",
    "with open(EXPERIMENTAL_DATA_FILE_PATH, 'r') as f:\n",
    "    # experimental_data = pd.read_json(f, orient='index')\n",
    "    names = ('Molecule ID', 'logP mean', 'logP SEM',\n",
    "             'Assay Type', 'Experimental ID', 'Isomeric SMILES')\n",
    "    experimental_data = pd.read_csv(f, names=names, skiprows=1)\n",
    "\n",
    "# Convert numeric values to dtype float.\n",
    "for col in experimental_data.columns[1:7]:\n",
    "        experimental_data[col] = pd.to_numeric(experimental_data[col], errors='coerce')\n",
    "\n",
    "\n",
    "experimental_data.set_index(\"Molecule ID\", inplace=True)\n",
    "experimental_data[\"Molecule ID\"] = experimental_data.index\n",
    "print(\"Experimental data: \\n\", experimental_data)\n",
    "\n",
    "# Import user map.\n",
    "with open('../predictions/SAMPL6-user-map-logP.csv', 'r') as f:\n",
    "    user_map = pd.read_csv(f)\n",
    "\n",
    "# Load submissions data.\n",
    "submissions_logP = load_submissions(LOGP_SUBMISSIONS_DIR_PATH, user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logP mean</th>\n",
       "      <th>logP SEM</th>\n",
       "      <th>logP model uncertainty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Molecule ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SM02</th>\n",
       "      <td>4.03</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM04</th>\n",
       "      <td>3.73</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM07</th>\n",
       "      <td>3.09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM08</th>\n",
       "      <td>2.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM09</th>\n",
       "      <td>3.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM11</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM12</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM13</th>\n",
       "      <td>3.57</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM14</th>\n",
       "      <td>2.12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM15</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM16</th>\n",
       "      <td>3.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             logP mean  logP SEM  logP model uncertainty\n",
       "Molecule ID                                             \n",
       "SM02              4.03      0.31                    0.31\n",
       "SM04              3.73      0.32                    0.31\n",
       "SM07              3.09      0.29                    0.31\n",
       "SM08              2.53      0.46                    0.31\n",
       "SM09              3.27      0.29                    0.31\n",
       "SM11              1.21      0.36                    0.31\n",
       "SM12              3.82      0.33                    0.31\n",
       "SM13              3.57      0.23                    0.31\n",
       "SM14              2.12      0.23                    0.31\n",
       "SM15              2.27      0.40                    0.31\n",
       "SM16              3.32      0.29                    0.31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_logP[0].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_logP[0].data.loc['SM08'][\"logP mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD-OPLSAA-dryoct 8.55\n",
      "YANK-GAFF-tip3p-ForceBalance-wet 9.41\n",
      "YANK-GAFF-tip3p-wet 7.96\n",
      "MD-OPLSAA-wetoct 7.59\n"
     ]
    }
   ],
   "source": [
    "for item in submissions_logP:\n",
    "    if float(item.data.loc['SM08'][\"logP mean\"]) > 7:\n",
    "        #print(item.name)\n",
    "        print(item.name, item.data.loc['SM08'][\"logP mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sampl]",
   "language": "python",
   "name": "conda-env-sampl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
