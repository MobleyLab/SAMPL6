#!/usr/bin/env python

# =============================================================================
# GLOBAL IMPORTS
# =============================================================================

import os
import time
import json
import glob
import logging

import numpy as np
import pandas as pd
from simtk import unit

from pkganalysis.submission import load_submissions
from pkganalysis.unbiasedanalyzer import analyze_directory, BARAnalyzer, InstantaneousWorkAnalyzer
from pkganalysis.sampling import (SamplingSubmission, energy_evaluations_iteration_cutoffs,
                                  cpu_time_iteration_cutoffs, get_iteration_cutoffs,
                                  YANK_N_ITERATIONS)


logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


# =============================================================================
# CONSTANTS
# =============================================================================

# Paths.
SCRIPT_DIR_PATH = os.path.dirname(os.path.realpath(__file__))

# Paths to submissions.
SUBMISSIONS_DIR_PATH = os.path.join(os.path.dirname(SCRIPT_DIR_PATH), 'Submissions')
SAMPLING_SUBMISSIONS_DIR_PATH = os.path.join(SUBMISSIONS_DIR_PATH, '975')
SUBMISSIONS_USER_MAP_FILE_PATH = os.path.join(SUBMISSIONS_DIR_PATH, 'SAMPL6_user_map.csv')

# The directory containing YANK output data.
YANK_SAMPLING_DIR_PATH = os.path.join(os.path.dirname(SCRIPT_DIR_PATH), 'SAMPLing')

# Output paths of the files generated by this analysis.
ANALYSIS_DIR_PATH = os.path.join(SCRIPT_DIR_PATH, 'YankAnalysis')
SAMPLING_ANALYSIS_DIR_PATH = os.path.join(ANALYSIS_DIR_PATH, 'Sampling')
YANK_CPU_TIMES_FILE_PATH = os.path.join(SAMPLING_ANALYSIS_DIR_PATH, 'yank_cpu_times.json')

# This was estimated from the average timings of 10 non-checkpoint iterations of a
# YANK simulation using the mixing replicas and I/O writing as serial bottlenecks.
YANK_PARALLEL_EFFICIENCY = {'complex': 0.87, 'solvent': 0.92}

# All system ids.
SYSTEM_IDS = [
    'CB8-G3-0', 'CB8-G3-1', 'CB8-G3-2', 'CB8-G3-3', 'CB8-G3-4',
    'OA-G3-0', 'OA-G3-1', 'OA-G3-2', 'OA-G3-3', 'OA-G3-4',
    'OA-G6-0', 'OA-G6-1', 'OA-G6-2', 'OA-G6-3', 'OA-G6-4'
]

# Systems, phases, and first iteration that used 8 GPUs instead of 4.
YANK_8GPUS = {
    'CB8-G3-0': {'complex': 0, 'solvent': 20001},
    'CB8-G3-1': {'complex': 0, 'solvent': 20001},
    'CB8-G3-2': {'complex': 0},
    'CB8-G3-3': {'complex': 0, 'solvent': 20001},
    'CB8-G3-4': {'complex': 0, 'solvent': 20001},
    'OA-G3-1': {'complex': 0},
    'OA-G6-2': {'complex': 0},
}

# Restraint distance cutoff that defines the binding site. These were obtained
# as the 99.99-percentile of the restraint distances in the bound state. The
# maximum across the 5 replicates was taken.
RESTRAINT_DISTANCE_CUTOFFS = {
    'CB8-G3': 0.458306729794 * unit.nanometers,
    'OA-G3': 0.5773037076 * unit.nanometers,
    'OA-G6': 0.606282174587 * unit.nanometers,
}


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def get_system_experiment_dir(system_id):
    """Return the path to YANK output directory for a system."""
    subsubdir = system_id.replace('-', '')
    subdir = 'experiment-' + subsubdir[:-1]
    return os.path.join(YANK_SAMPLING_DIR_PATH, 'experiments', subdir, subsubdir)


# =============================================================================
# EXTRACT YANK CPU TIME
# =============================================================================

def diff_timestamps(timestamp1, timestamp2):
    """Compute time delta in seconds from the two string timestamps."""
    t1 = time.mktime(time.strptime(timestamp1))
    t2 = time.mktime(time.strptime(timestamp2))
    return t2 - t1


def extract_yank_cpu_time(system_id):
    """Extract the total CPU/GPU simulation time from a YANK simulation NC file."""
    from yank.repex import Reporter

    def time_str(time_in_seconds):
        return '{:.2f}s ({:.2f}h, {:.2f}d)'.format(time_in_seconds, time_in_seconds/3600,
                                                   time_in_seconds/3600/24)

    total_time = 0.0
    output_dir_path = get_system_experiment_dir(system_id)

    for phase_name in ['complex', 'solvent']:
        phase_time = 0.0
        nc_file_path = os.path.join(output_dir_path, phase_name + '.nc')

        # Read all the timestamps.
        reporter = None
        try:
            reporter = Reporter(nc_file_path, open_mode='r')
            timestamps = reporter.read_timestamp()
        finally:
            if reporter is not None:
                reporter.close()
        timestamps = timestamps.tolist()
        assert len(timestamps) == YANK_N_ITERATIONS + 1

        # Separate 4 GPUs from 8 GPUs calculations.
        try:
            switch_iteration = YANK_8GPUS[system_id][phase_name]
        except KeyError:
            # All the calculation with 4 GPUs.
            timestamps = {4: timestamps, 8: []}
        else:
            timestamps = {4: timestamps[:switch_iteration],
                          8: timestamps[switch_iteration:]}

        # Compute the average mean duration ignoring interruptions.
        mean_iteration_durations = {}
        for n_gpus, stamps in timestamps.items():
            if len(stamps) == 0:
                continue

            # Compute time deltas.
            timedeltas = [diff_timestamps(stamps[i-1], stamps[i]) for i in range(1, len(stamps))]

            # Detect interruptions by finding iterations that > 10 times the median.
            median_delta = np.median(timedeltas)
            interruption_iterations = [i for i in range(len(timedeltas))
                                       if timedeltas[i] > 10 * median_delta]
            logger.info('Detected interruptions for {}/{} at '
                        'iterations {}'.format(system_id, phase_name, interruption_iterations))

            # Compute average iteration duration.
            mean_iteration_durations[n_gpus] = np.mean([t for i, t in enumerate(timedeltas)
                                                        if i not in interruption_iterations])

            # Compute total wallclock time.
            interruption_iterations = interruption_iterations + [len(stamps)-1]
            for interruption_idx, interruption_iteration in enumerate(interruption_iterations):
                if interruption_idx == 0:
                    timestamp1 = stamps[0]
                else:
                    # Use mean iteration duration to fill the previous interruption.
                    phase_time += n_gpus * mean_iteration_durations[n_gpus]
                    timestamp1 = stamps[interruption_iterations[interruption_idx-1] + 1]
                timestamp2 = stamps[interruption_iteration]
                phase_time += n_gpus * diff_timestamps(timestamp1, timestamp2)

        # Add extra iteration for calculations that switched from 4 to 8 GPUs.
        if len(mean_iteration_durations) == 2:
            phase_time += 8 * mean_iteration_durations[8]

        # Take into account parallelization efficiency.
        phase_time *= YANK_PARALLEL_EFFICIENCY[phase_name]
        logger.info('CPU time for phase {}/{}: {}'.format(system_id, phase_name, time_str(phase_time)))
        total_time += phase_time

    logger.info('Total CPU time for {}: {}'.format(system_id, time_str(total_time)))
    return total_time


def extract_all_yank_cpu_times():
    """Extract the total times of all YANK simulations.

    Returns
    -------
    total_times : dict
        The dictionary system_id: total_time (in seconds).

    """
    # Check if we have already calculated it.
    try:
        with open(YANK_CPU_TIMES_FILE_PATH, 'r') as f:
            total_times = json.load(f)
        logger.info('Found pre-computed YANK total times '
                    'in {}'.format(YANK_CPU_TIMES_FILE_PATH))
        return total_times
    except FileNotFoundError:
        pass

    # Extract all total times.
    logger.info('Computing YANK total times')
    total_times = {system_id: extract_yank_cpu_time(system_id) for system_id in SYSTEM_IDS}

    # Save the result.
    with open(YANK_CPU_TIMES_FILE_PATH, 'w') as f:
        json.dump(total_times, f, indent=4, sort_keys=True)
    return total_times


# =============================================================================
# GENERATE ANALYSIS ITERATION CUTOFFS
# =============================================================================

def generate_iteration_cutoffs():
    """Read all the submissions costs and determine the corresponding YANK iterations."""
    # Import user map and submissions.
    with open(SUBMISSIONS_USER_MAP_FILE_PATH, 'r') as f:
        user_map = pd.read_csv(f)
    submissions = load_submissions(SamplingSubmission, SAMPLING_SUBMISSIONS_DIR_PATH, user_map)

    # Extract YANK CPU times.
    yank_cpu_times = extract_all_yank_cpu_times()

    # Generate all iterations that need to be analyzed.
    systems_iteration_cutoffs = {system_id: set() for system_id in SYSTEM_IDS}
    for submission in submissions:
        for system_id, row in submission.cost.iterrows():
            system_name = system_id[:-2]

            # Generate the 100 iteration cutoffs splitting the YANK calculation by energy evaluations.
            n_energy_evaluations = row['N energy evaluations']
            iteration_cutoffs = energy_evaluations_iteration_cutoffs(n_energy_evaluations, system_name)
            systems_iteration_cutoffs[system_id].update(iteration_cutoffs)

            # For GROMACS-EE, analyze also the iterations corresponding to energy evaluations
            # resulting from adding the full cost of the equilibration stage to each replica.
            if submission.paper_name == 'GROMACS/EE' and system_name != 'CB8-G3':
                mean_free_energies = submission.mean_free_energies()
                mean_data = mean_free_energies[mean_free_energies['System name'] == system_name]
                first_nonzero_idx = np.nonzero(mean_data['$\Delta$G [kcal/mol]'].values)[0][0]
                calibration_cost = mean_data['N energy evaluations'].values[first_nonzero_idx] * 4
                n_energy_evaluations += calibration_cost
                iteration_cutoffs = energy_evaluations_iteration_cutoffs(n_energy_evaluations, system_name,
                                                                         start=calibration_cost)
                systems_iteration_cutoffs[system_id].update(iteration_cutoffs)

            # Use CPU time when available and fallback on wall-clock time if not.
            tot_time = row['CPU time'] if not np.isnan(row['CPU time']) else row['Wall clock time']
            # Generate the 100 iteration cutoffs splitting the YANK calculation by CPU time.
            try:
                iteration_cutoffs = cpu_time_iteration_cutoffs(tot_time, system_id, yank_cpu_times)
            except AssertionError:
                print('Skipping iterations corresponding to CPU time for submission {}, system {} '
                      'with total CPU time {}'.format(submission.receipt_id, system_id, tot_time))
            else:
                systems_iteration_cutoffs[system_id].update(iteration_cutoffs)

    # Add iteration cutoffs for all 40000 iterations.
    for system_id, iteration_cutoffs in systems_iteration_cutoffs.items():
        iteration_cutoffs.update(get_iteration_cutoffs(YANK_N_ITERATIONS))

    # Convert sets to ordered lists.
    for system_id, iteration_cutoffs in systems_iteration_cutoffs.items():
        systems_iteration_cutoffs[system_id] = sorted(iteration_cutoffs)
    return systems_iteration_cutoffs


def get_computed_free_energies(system_id, dir_path=SAMPLING_ANALYSIS_DIR_PATH, file_prefix='yank', cleanup=False):
    """Read data and cleanup jobid files.

    Parameters
    ----------
    file_prefix: str, optional
        The prefix of the job id files without system name and jobid.

    """
    # Read all free energies.
    file_base_path = os.path.join(dir_path, '{}-{}'.format(file_prefix, system_id))
    output_file_path = file_base_path + '.json'

    # Read current merged file.
    free_energies = {}
    if os.path.exists(output_file_path):
        with open(output_file_path, 'r') as f:
            free_energies.update(json.load(f))

    # Merge parallel jobs output.
    for file_path in glob.glob(file_base_path + '-*.json'):
        with open(file_path, 'r') as f:
            free_energies.update(json.load(f))

    # Delete parallel job files.
    if cleanup:
        # Write free energies merged result.
        with open(output_file_path, 'w') as f:
            json.dump(free_energies, f, indent=4, sort_keys=True)

        # Remove jobid files.
        for file_path in glob.glob(file_base_path + '-*.json'):
            os.remove(file_path)

    # Convert iterations from string to integers.
    try:
        # If k are just iterations, convert the strings to integers.
        return {int(k): v for k, v in free_energies.items()}
    except ValueError:
        # In this case, free_energies is the decomposition data.
        return free_energies


# =============================================================================
# MAIN
# =============================================================================

def analyze_yank(jobid, jobspersystem, cleanup=False, dry_run=False, analyzer_class=None,
                 output_dir_path=SAMPLING_ANALYSIS_DIR_PATH, file_prefix='yank',
                 iteration_cutoffs=None, print_iterations_to_compute=True, **analyzer_kwargs):
    """Analyze YANK at all the submitted wall-clock times/number of energy evaluations."""
    # Determine the iterations at which to analyze the YANK calculations.
    if iteration_cutoffs is None:
        systems_iteration_cutoffs = generate_iteration_cutoffs()
    else:
        systems_iteration_cutoffs = {system_id: iteration_cutoffs for system_id in SYSTEM_IDS}

    # Load iterations that have been already calculated.
    for system_id, iteration_cutoffs in systems_iteration_cutoffs.items():
        computed_iterations = get_computed_free_energies(system_id, dir_path=output_dir_path,
                                                         file_prefix=file_prefix, cleanup=cleanup)
        # Compute only the iterations that we haven't computed.
        iterations_to_compute = set(iteration_cutoffs) - set(computed_iterations)
        systems_iteration_cutoffs[system_id] = sorted(iterations_to_compute)
    # Remove System IDs with no iterations to compute.
    systems_iteration_cutoffs = {k: v for k, v in systems_iteration_cutoffs.items() if len(v) > 0}

    # If this is a dry run, just print how many iterations we need to analyze for each system.
    if dry_run:
        if print_iterations_to_compute:
            for k, v in systems_iteration_cutoffs.items():
                print(k, len(v))
        return systems_iteration_cutoffs

    # Split all iteration cutoffs for jobs.
    jobs_systems_iteration_cutoffs = []
    for system_id, iteration_cutoffs in systems_iteration_cutoffs.items():
        for i, cutoffs in enumerate(np.array_split(iteration_cutoffs, jobspersystem)):
            system_job_id = system_id + '-' + str(i)
            jobs_systems_iteration_cutoffs.append((system_id, system_job_id, cutoffs.tolist()))

    # Determine iterations assigned to this job.
    if jobid:
        n_jobs = len(jobs_systems_iteration_cutoffs)
        jobs_to_run = [jobs_systems_iteration_cutoffs[jobid - 1]]
        logger.info('Running job id {}/{} for a total of {} iterations.'
                    ''.format(jobid, n_jobs, len(jobs_to_run[0][-1])))
    else:
        jobs_to_run = jobs_systems_iteration_cutoffs

    # Run analyzer.
    for system_id, system_job_id, iteration_cutoffs in jobs_to_run:
        logger.info('Computing free energy profile of {}'.format(system_job_id))
        system_name = system_id[:-2]
        experiment_directory = get_system_experiment_dir(system_id)
        distance_cutoff = RESTRAINT_DISTANCE_CUTOFFS[system_name]

        # Analyze all iterations.
        free_energies = analyze_directory(experiment_directory,
                                          distance_cutoffs=distance_cutoff,
                                          iteration_cutoffs=iteration_cutoffs,
                                          analyzer_class=analyzer_class,
                                          **analyzer_kwargs)

        # Store analysis results.
        free_energies = {i: f for i, f in zip(iteration_cutoffs, free_energies)}
        output_file_path = os.path.join(output_dir_path, '{}-{}.json'.format(file_prefix, system_job_id))
        with open(output_file_path, 'w') as f:
            json.dump(free_energies, f, indent=4, sort_keys=True)

    return systems_iteration_cutoffs


def analyze_yank_restraint(system_id):
    """Analyze YANK at different restraint distance cutoffs."""
    # Infer the experiment directory. The folder names have no hashes.
    system_id = system_id.replace('-', '')
    store_dir_path = os.path.join('..', 'SAMPLing', 'experiments',
                                  'experiment-'+system_id[:-1], system_id)

    # We recompute the free energy at intervals of 1A from 3 to 15A.
    restraint_distance_cutoffs = np.linspace(3.0, 15.0, num=13) * unit.angstrom
    free_energies = analyze_directory(store_dir_path, distance_cutoffs=restraint_distance_cutoffs)

    # Store analysis results.
    free_energies = {c: f for c, f in zip(restraint_distance_cutoffs / unit.angstrom, free_energies)}
    restraint_analysis_dir_path = os.path.join(ANALYSIS_DIR_PATH, 'RestraintAnalysis')
    os.makedirs(restraint_analysis_dir_path, exist_ok=True)
    output_file_path = os.path.join(restraint_analysis_dir_path, '{}.json'.format(system_id))
    with open(output_file_path, 'w') as f:
        json.dump(free_energies, f, indent=4, sort_keys=True)


def analyze_yank_bias(jobid, jobspersystem, cleanup=False, dry_run=False, analyzer_class=None):
    """Analyze YANK discarding the initial iterations to see if it helps with the bias."""
    # starting_iterations = [250, 500, 1000] + list(range(2000, 20000, 2000))
    starting_iterations = [0, 5, 10, 25, 50, 100, 250, 500, 1000, 2000, 4000, 8000, 16000, 24000, 32000]

    # Obtain the iterations that have been already analyzed by YANK.
    # Using only these allow us to compare the subset to the full simulation later.
    systems_iteration_cutoffs = generate_iteration_cutoffs()

    # Save bias analysis with BAR in a separate directory.
    if analyzer_class is not None and analyzer_class.__name__ == 'BARAnalyzer':
        subdir_name = 'BiasAnalysis-BAR'
    else:
        subdir_name = 'BiasAnalysis'

    # Analyze YANK free energy trajectory discarding progressively more and more data.
    # Here we accumulate the jobs that we need to run before distributing them.
    iterations_to_analyze = {}
    for starting_iteration in starting_iterations:
        # Determine the 100 iterations to analyze discarding the initial starting_iteration.
        target_iterations = np.linspace(starting_iteration, YANK_N_ITERATIONS, 101)
        target_iterations = [int(x) for x in target_iterations[1:]]  # Convert to integers.

        # Create the analysis directory if it doesn't exist.
        bias_analysis_dir_path = os.path.join(ANALYSIS_DIR_PATH, subdir_name, 'iter' + str(starting_iteration))
        os.makedirs(bias_analysis_dir_path, exist_ok=True)

        # Determine 100 iterations starting from starting_iteration that have been already analyzed.
        for system_id, iteration_cutoffs in systems_iteration_cutoffs.items():
            # Retrieve free energies that have been already computed and cleanup.
            analyzed_iterations = get_computed_free_energies(system_id, dir_path=bias_analysis_dir_path,
                                                             cleanup=cleanup)
            # If we cleanup, cleanup also decomposition files.
            if cleanup:
                get_computed_free_energies(system_id, dir_path=bias_analysis_dir_path,
                                           file_prefix='fe-decomposition', cleanup=True)
            iteration_indices = np.searchsorted(iteration_cutoffs, target_iterations)
            # Transform to a set to make sure we don't have duplicate iterations.
            iterations = sorted(set([int(iteration_cutoffs[i]) for i in iteration_indices]))
            # Don't recompute iterations that we have already analyzed.
            iterations = [i for i in iterations if i not in analyzed_iterations]
            if len(iterations) > 0:
                iterations_to_analyze[(starting_iteration, system_id)] = iterations

    # If this is a dry run, just print how many iterations we need to analyze for each system.
    if dry_run:
        for k, v in iterations_to_analyze.items():
            print(k, len(v))
        return

    # Split all iteration cutoffs for jobs.
    jobs_iterations_to_analyze = []
    for (starting_iteration, system_id), iteration_cutoffs in iterations_to_analyze.items():
        for i, cutoffs in enumerate(np.array_split(iteration_cutoffs, jobspersystem)):
            system_job_id = system_id + '-' + str(i)
            jobs_iterations_to_analyze.append((system_id, starting_iteration, system_job_id, cutoffs.tolist()))

    # Determine iterations assigned to this job.
    if jobid:
        n_jobs = len(jobs_iterations_to_analyze)
        jobs_to_run = [jobs_iterations_to_analyze[jobid - 1]]
        logger.info('Running job id {}/{} for a total of {} iterations.'
                    ''.format(jobid, n_jobs, len(jobs_to_run[0][-1])))
    else:
        jobs_to_run = jobs_iterations_to_analyze

    # Run analyzer.
    for system_id, starting_iteration, system_job_id, iteration_cutoffs in jobs_to_run:
        logger.info('Computing free energy profile of {} starting from iteration {}'.format(
            system_job_id, starting_iteration))
        system_name = system_id[:-2]
        experiment_directory = get_system_experiment_dir(system_id)
        distance_cutoff = RESTRAINT_DISTANCE_CUTOFFS[system_name]

        # Analyze all iterations.
        free_energies, decomposition = analyze_directory(experiment_directory,
                                                         n_discarded_initial_iterations=starting_iteration,
                                                         distance_cutoffs=distance_cutoff,
                                                         iteration_cutoffs=iteration_cutoffs,
                                                         analyzer_class=analyzer_class,
                                                         return_decomposition=True)

        # Store analysis results.
        free_energies = {i: f for i, f in zip(iteration_cutoffs, free_energies)}
        bias_analysis_dir_path = os.path.join(ANALYSIS_DIR_PATH, subdir_name, 'iter' + str(starting_iteration))
        output_file_path = os.path.join(bias_analysis_dir_path, 'yank-{}.json'.format(system_job_id))
        with open(output_file_path, 'w') as f:
            json.dump(free_energies, f, indent=4, sort_keys=True)

        # Store free energy decomposition data.
        output_file_path = os.path.join(bias_analysis_dir_path, 'fe-decomposition-{}.json'.format(system_job_id))
        with open(output_file_path, 'w') as f:
            json.dump(decomposition, f, indent=4, sort_keys=True)


def analyze_yank_correlation(jobid, jobspersystem, cleanup=False, dry_run=False):
    """Analyze YANK discarding the initial iterations to see if it helps with the bias."""
    statistical_inefficiencies = [2, 5, 10, 20, 50, 100, 200]
    output_dir_pattern = os.path.join(ANALYSIS_DIR_PATH, 'CorrelationAnalysis', 'statineff-{}')

    # Analyze only 100 iterations for each statistical inefficiency.
    iteration_cutoffs = get_iteration_cutoffs(YANK_N_ITERATIONS)

    # First run a dry run to figure out how many systems we need to compute for each statistical inefficiency.
    n_jobs_per_ineff = [0]
    for stat_inefficiency in statistical_inefficiencies:
        output_dir_path = output_dir_pattern.format(stat_inefficiency)
        os.makedirs(output_dir_path, exist_ok=True)
        systems_iteration_cutoffs = analyze_yank(jobid, jobspersystem, cleanup=cleanup, dry_run=True,
                                                 output_dir_path=output_dir_path, file_prefix='yank',
                                                 iteration_cutoffs=iteration_cutoffs,
                                                 print_iterations_to_compute=False,
                                                 fixed_statistical_inefficiency=stat_inefficiency)
        if dry_run:
            for k, v in systems_iteration_cutoffs.items():
                print(stat_inefficiency, k, len(v))
        n_jobs_per_ineff.append(len(systems_iteration_cutoffs) * jobspersystem)

    # Compute the cumulative number of jobs to identify which one we need to run.
    cum_n_jobs_per_ineff = np.array(n_jobs_per_ineff).cumsum()
    print('Total number of jobs:', sum(n_jobs_per_ineff))

    # If this is a dry run, there's nothing else to do.
    if dry_run:
        return

    # Identify statistical efficiency and jobid.
    for i, cum_n_jobs in enumerate(cum_n_jobs_per_ineff):
        if cum_n_jobs >= jobid:
            break
    stat_inefficiency = statistical_inefficiencies[i-1]
    jobid -= cum_n_jobs_per_ineff[i-1]

    output_dir_path = output_dir_pattern.format(stat_inefficiency)
    analyze_yank(jobid, jobspersystem, cleanup=cleanup, dry_run=dry_run,
                 output_dir_path=output_dir_path, file_prefix='yank',
                 iteration_cutoffs=iteration_cutoffs,
                 fixed_statistical_inefficiency=stat_inefficiency)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobid', type=int)
    parser.add_argument('--jobspersystem', type=int, default=1)
    args = parser.parse_args()

    # Run YANK analysis.
    analyze_yank(args.jobid, args.jobspersystem, cleanup=True, dry_run=True)
    # analyze_yank(args.jobid, args.jobspersystem, cleanup=True, dry_run=True,
    #              analyzer_class=InstantaneousWorkAnalyzer)

    # Collect data for YANK restraint sensitivity analysis.
    # system_id = SYSTEM_IDS[args.jobid-1]
    # analyze_yank_restraint(system_id)

    # Collect data for YANK bias analysis.
    # analyze_yank_bias(args.jobid, args.jobspersystem, cleanup=True, dry_run=True)

    # Collect data for YANK correlation analysis.
    # analyze_yank_correlation(args.jobid, args.jobspersystem, cleanup=True, dry_run=True)
